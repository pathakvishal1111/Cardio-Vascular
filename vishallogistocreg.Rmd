---
title: "logisticcardio"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r dataload}
library(ggplot2)


cd = read.csv("C:/DS/introduction to datascience/Project Cardio/cardio.csv")

dim(cd)

cd$age<- (cd$age/365)

cd$age<- round(cd$age) 
```


```{r basicchecks}

#Structure
str(cd)

#Summary
summary(cd)

# Anomalies found in height, weight, systolic (ap_hi) and diastolic (ap_lo)
#Gender,cholestrol,gluc,smoke,alco,active,cardio are needed to be converted into categorical variable

```
```{r EDA }

library(pastecs)
library(psych)
library(Hmisc)


```

```{r Outlierdetectionofage}
fivenum(cd$age)
psych::describe(cd$age)
stat.desc(cd$age)
hist(cd$age,
     main = "Histogram of Age",
     xlab = "Age in Years")
boxplot(cd$age,
        main = toupper("Boxplot of Age"),
        ylab = "Age in years",
        col = "blue")
outlier_values <- boxplot.stats(cd$age)$out  # outlier values.
boxplot(cd$age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

```
As we can see skewness in -0.3 which means age is approximately symmetric. As if skewness is in between scale of (-0.5 to 0.5)it will be considered approximately symmetric.


## so in age as we can see the outliers are value 30 which are outside of 1.5*IQR . We are going to use capping methiod in order to treat outlier. For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile.

```{r Outliertreatmentofage}
qnt <- quantile(cd$age, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$age, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$age, na.rm = T)
cd$age[cd$age < (qnt[1] - H)] <- caps[1]
cd$age[cd$age > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$age)$out  # outlier values.
boxplot(cd$age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

``` 
# As we can see now the age column has no outliers.we are good to go.





```{r genderEDA}
psych::describe(cd$gender)
fivenum(cd$gender)
describe(cd$gender)
stat.desc(cd$gender)


```
##As we can see there are no outliers in gender variable

```{r HeightEDA}
fivenum(cd$height)
psych::describe(cd$height)
stat.desc(cd$height)

hist(cd$height,
     main = "Histogram of Height",
     xlab = "Height in centimeter")

outlier_values <- boxplot.stats(cd$cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$height))

```
## As we can see there are few of outliers in Height variable , which is around .004 percent of total data. since this is continous data so we are going treat with capping method. For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile. Moreover the skewness is around -0.63 which means height column is left skewed.

```{r Heightoutlierimpute}

qnt <- quantile(cd$height, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$height, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$height, na.rm = T)
cd$height[cd$height < (qnt[1] - H)] <- caps[1]
cd$height[cd$height > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

```
## As we can see now there are no outliers in height column.Also  earlier there was skewness in the column and it was (-0.63) but after the outlier treatment it reduces to 0.09 . so height variable is not skewed.so we are good to go.


```{r WeightEDA}
fivenum(cd$weight)
psych::describe(cd$weight)
stat.desc(cd$weight)

hist(cd$weight,
     main = "Histogram of Weight",
     xlab = "Weight in kg")

outlier_values <- boxplot.stats(cd$weight)$out  # outlier values.
boxplot(cd$weight, main="Weight", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$weight))

```
##As we can see there are 10 % of data which are outlier . Also data is highly asymmetric as skew value is (1.01) which meamns age is highly right skewed.we will first treat outlier ,then we will check if skewness persists then we will apply transformation on the weight column.



```{r Weightoutlierimpute}

qnt <- quantile(cd$weight, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$weight, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$weight, na.rm = T)
cd$weight[cd$weight < (qnt[1] - H)] <- caps[1]
cd$weight[cd$weight > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$weight)

```
## As we can see now there are no outliers and also skewness is 0.39 which means our data is approximately normal.we are good to go.

```{r ap_hi(EDA)}
fivenum(cd$ap_hi)
psych::describe(cd$ap_hi)
stat.desc(cd$ap_hi)

hist(cd$ap_hi,
     main = "Histogram of systolic Blood Pressure",
     xlab = "Ap_hi")

outlier_values <- boxplot.stats(cd$ap_hi)$out  # outlier values.
boxplot(cd$ap_hi, main="systolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$ap_hi))

```
## As we can see there is a 8 units of difference in mean and Meadian.which is really high . But the outliers are only around .05 % . This means that the values of outliers are of very values. Aslo the skew and kurtosis (85.29,7579.32)values are really very high. which again point to the fact that some values are incorrect or garbage values . we are to going treat the outliers in the next section and then we will analyse our column again.

```{r SBPoutlierimpute}

qnt <- quantile(cd$ap_hi, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$ap_hi, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$ap_hi, na.rm = T)
cd$ap_hi[cd$ap_hi < (qnt[1] - H)] <- caps[1]
cd$ap_hi[cd$ap_hi > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$ap_hi)$out  # outlier values.
boxplot(cd$ap_hi, main="systolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$ap_hi)

```

## yes we were absolutely right the outliers were of very high and they were garbage value.After treating outliers , we can see our sweet boxplot(without any outliers) .Moreover our skew value is now under approximately normal range which is 0.54.

```{r ap_low(EDA)}
fivenum(cd$ap_lo)
psych::describe(cd$ap_lo)
stat.desc(cd$ap_lo)

hist(cd$ap_lo,
     main = "Histogram of Diastolic Blood Pressure",
     xlab = "Ap_low")

outlier_values <- boxplot.stats(cd$ap_lo)$out  # outlier values.
boxplot(cd$ap_lo, main="Diastolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$ap_lo))

```

## As we can see there is difference of 16 units in mean and median.From the histogram we can see that data is right skewed which is around (32.11). Also the kurtosisis really high. there are around 10% outliers so we are going to treat it.after that we are going to check again

```{r DBDoutliertreatment}

qnt <- quantile(cd$ap_lo, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$ap_lo, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$ap_lo, na.rm = T)
cd$ap_lo[cd$ap_lo < (qnt[1] - H)] <- caps[1]
cd$ap_lo[cd$ap_lo > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$ap_lo)$out  # outlier values.
boxplot(cd$ap_lo, main="Diastolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$ap_lo)

```
## As we can see now after treatment of outliers , skew has also been in expected range(0.48) . now our variable is approximately normally distributed.

```{r cholestrolEDA}

psych::describe(cd$cholesterol)
fivenum(cd$cholesterol)
describe(cd$cholesterol)
stat.desc(cd$cholesterol)
```
## As we can see there are no missing variables in this columns. category 1 has maximum values followed by category 2.

```{r glucEDA}

psych::describe(cd$gluc)
fivenum(cd$gluc)
describe(cd$gluc)
stat.desc(cd$gluc)
```
## It has no missing values and most of the values are under category 1(normal) followed by category 2(above normal)

```{r glucEDA}

psych::describe(cd$smoke)
fivenum(cd$smoke)
describe(cd$smoke)
stat.desc(cd$smoke)
```

## Smoking column also has no missing values . . so we will ignore here . but it a category has very less value as compared to other we might consider it as outlier.

```{r alcoholEDA}

psych::describe(cd$alco)
fivenum(cd$alco)
describe(cd$alco)
stat.desc(cd$alco)
```
## No missing values or junk values in alcohol column as well.

```{r alcoholEDA}

psych::describe(cd$cardio)
fivenum(cd$cardio)
describe(cd$cardio)
stat.desc(cd$cardio)
```
## the suprising things about this column is that it has zero skew value , though it doesnt affect our anlaysis, but it means data is perfectly distributed.

#Missing Value check and treatment on continous variables
```{r Missingvalues}

sum(is.na(cd$age))
sum(is.na(cd$height))
sum(is.na(cd$weight))
sum(is.na(cd$ap_hi))
sum(is.na(cd$ap_lo))

```

##There are no missing variables in the data , which means that we are good to go for our bivariate analysis.



```{r Checkaphivsaplo}

temp_min = pmin(cd$ap_hi, cd$ap_lo)
cd$ap_hi = pmax(cd$ap_hi, cd$ap_lo)
cd$ap_lo = temp_min
```
## since diastolic and systolic are connected and diastolic will always be greater than systolic . so for sanity we have implement few changes to swap values if sysytolic is less than diastolic


```{r calculateBMI}

cd$bmi<- (cd$weight*10000)/(cd$height*cd$height) # Weight in kgs and height in cms
cd$bmi1 <- cd$weight/(cd$height*cd$height)*10000


```


```{r subsettingforfactor}
library(GPArotation)
l1 <- data.frame(cd$age ,cd$height,cd$weight , cd$ap_hi,cd$ap_lo , cd$bmi)
corrm<- cor(l1)     
```

```{r screeplot}
scree(corrm, factors=TRUE, pc=TRUE, main="scree plot", hline=NULL, add=FALSE) ### SCREE PLOT
eigen(corrm)$values                                                     ### EIGEN VALUES

```


## As we can see the scree plot there are only 3 values which have eigen value above 1 , this means that we are going to use factor analysis for PC3.

```{r cumvarvar}
library(plyr)
eigen_values <- mutate(data.frame(eigen(corrm)$values)
                       ,cum_sum_eigen=cumsum(eigen.corrm..values)
                       , pct_var=eigen.corrm..values/sum(eigen.corrm..values)
                       , cum_pct_var=cum_sum_eigen/sum(eigen.corrm..values))
```

```{r factor analysis}
library(psych)
FA<-fa(r=corrm, 3, rotate="varimax", SMC=FALSE, fm="minres")
print(FA)                                                  
FA_SORT<-fa.sort(FA)                                        
ls(FA_SORT)                                                 
FA_SORT$loadings
```

## From factor analysis we have taken 3 factors in account as per factor loadings. and those are bmi , api_hi , height. So we are good to go for our model implementation.


```{r converting into factors}

#Gender
cd$gender<-as.factor(cd$gender)
#Cholestrol
cd$cholesterol<-as.factor(cd$cholesterol)
#Glucose
cd$gluc<-as.factor(cd$gluc)
#Smoke
cd$smoke<- as.factor(cd$smoke)
#Alcohol
cd$alco<- as.factor(cd$alco)
#Active
cd$active<- as.factor(cd$active)
#Cardio
cd$cardio<- as.numeric(cd$cardio)

levels(cd$gender)
levels(cd$cholesterol)
levels(cd$gluc)
levels(cd$smoke)
levels(cd$alco)
levels(cd$active)
```

## so As we can see there are more than 2 levels in cholesterol and glucose so in order to have better fit we need to implement dummy variables.

```{r dummy variables}
cd$cholnormal  <- ifelse(cd$cholesterol == 1, 1, 0)
cd$cholabovenorm <- ifelse(cd$cholesterol == 2, 1, 0)
sum(cd$cholabovenorm)
sum(cd$cholnormal)

cd$glucnorm <- ifelse(cd$gluc == 1, 1, 0)
cd$glucabovenorm <- ifelse(cd$gluc == 2, 1, 0)
sum(cd$glucnorm)
sum(cd$glucabovenorm)

```

## so we have create 2 dummy variables for cholestrol and 2 dummy variaables for glucose category and in both the columns there are 3 levels


```{r dividedata}
require(caTools) #install caTools
set.seed(123)
sample = sample.split(cd,SplitRatio = 0.70)
train_cd =subset(cd,sample ==TRUE)
test_cd =subset(cd, sample==FALSE)
```

## we are dividibg the data into test and train.

```{r logisticmodel}

fit <- glm(cardio ~ bmi+weight+ap_hi+height+glucnorm+glucabovenorm+smoke+alco+active+cholnormal+cholabovenorm , data = train_cd , family = binomial(logit) )
summary(fit)
```
## As you can see the factors we have used to build model is giving results . For most of the variables the p value is low that means they are effective in predicting the cario deseas e. bmi and height are 2 variables which are not significant . alos the diffrence in value of null deviance and residual deviance is high . which means model is fitting.

```{r letscheck}

require(MASS)
step3<- stepAIC(fit,direction="both")
?stepAIC()
ls(step3)
step3$anova

```   


## As we can see the step aic functio has removed bmi , from the model and gave our best predictors.Now we gonna run our model again by removing BMI and compare it as well.


```{r secondmodel}
fit2<-glm(cardio ~ weight + ap_hi + height + glucnorm + glucabovenorm + 
    smoke + alco + active + cholnormal + cholabovenorm , data = train_cd , family = binomial(logit) )
summary(fit2)
```
## As we can see there AIC value for 1st and second model is almost same which means BMI was not impacting the model.

```{r letscheckhl}
library(ResourceSelection)
hl <- hoslem.test(train_cd$cardio, fitted(fit2), g = 10)

cbind(hl$expected , hl$observed)

```


```{r letscheckconcordance}
Concordance = function(GLM.binomial) {
  outcome_and_fitted_col = cbind(GLM.binomial$y, GLM.binomial$fitted.values)
  # get a subset of outcomes where the event actually happened
  ones = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 1,]
  # get a subset of outcomes where the event didn't actually happen
  zeros = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 0,]
  # Equate the length of the event and non-event tables
  if (length(ones[,1])>length(zeros[,1])) {ones = ones[1:length(zeros[,1]),]}
  else {zeros = zeros[1:length(ones[,1]),]}
  # Following will be c(ones_outcome, ones_fitted, zeros_outcome, zeros_fitted)
  ones_and_zeros = data.frame(ones, zeros)
  # initiate columns to store concordant, discordant, and tie pair evaluations
  conc <- rep(NA, length(ones_and_zeros[,1]))
  disc <- rep(NA, length(ones_and_zeros[,1]))
  ties = rep(NA, length(ones_and_zeros[,1]))
  for (i in 1:length(ones_and_zeros[,1])) {
    # This tests for concordance
    if (ones_and_zeros[i,2] > ones_and_zeros[i,4])
    {conc[i] = 1
    disc[i] = 0
    ties[i] = 0}
    # This tests for a tie
    else if (ones_and_zeros[i,2] == ones_and_zeros[i,4])
    {
      conc[i] = 0
      disc[i] = 0
      ties[i] = 1
    }
    # This should catch discordant pairs.
    else if (ones_and_zeros[i,2] < ones_and_zeros[i,4])
    {
      conc[i] = 0
      disc[i] = 1
      ties[i] = 0
    }
  }
  # Here we save the various rates
  conc_rate = mean(conc, na.rm=TRUE)
  disc_rate = mean(disc, na.rm=TRUE)
  tie_rate = mean(ties, na.rm=TRUE)
  Somers_D<-conc_rate - disc_rate
  gamma<- (conc_rate - disc_rate)/(conc_rate + disc_rate)
  #k_tau_a<-2*(sum(conc)-sum(disc))/(N*(N-1)
  return(list(concordance=conc_rate, num_concordant=sum(conc), discordance=disc_rate, num_discordant=sum(disc), tie_rate=tie_rate,num_tied=sum(ties),
              somers_D=Somers_D, Gamma=gamma))
  
}
Concordance(fit2) 
require(car)
vif(fit2)

  
```

##adj-R square would be totally irrelevant in case of logistic regression because we model the log odds ratio and it becomes very difficult in terms of explain ability.This is where concordance steps in to help. Concordance tells us the association between actual values and the values fitted by the model in percentage terms. Concordance is defined as the ratio of number of pairs where the 1 had a higher model score than the model score of zero to the total number of 1-0 pairs possible. A higher value for concordance (60-70%) means a better fitted model. However, a very large value for concordance (85-95%) could also suggest that the model is over-fitted and needs to be re-aligned to explain the entire population. so as we can see our model have 77 % of concordance whcih means its a good fit.

##Moving over somers d if you see it is around 0.55 . Somers’ D is an index that you want to be closer to 1 and farther from −1. So we can say that our model is good fitted .

## Now we are going to see the AUC And KS statsics for which we will need ROCR Package.

```{r Predict }

pred <- predict(fit2 , newdata = test_cd,type = 'response')
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- test_cd$cardio


mean(y_pred == y_act)
```
## As we can see in prediction we have an accuracy rate of 72 %

```{r completely}
library(sqldf)
t1<-cbind(train_cd, Prob=predict(fit2,train_cd , type="response" ))
names(t1)

mean(t1$Prob)
View(t1)

t2<-cbind(test_cd,  Prob=predict(fit2,test_cd, type="response"))  

mean(t2$Prob)
View(t2)

#Decile Analysis Reports - t1(training)

# find the decile locations 
decLocations <- quantile(t1$Prob, probs = seq(0.1,0.9,by=0.1))

# use findInterval with -Inf and Inf as upper and lower bounds
t1$decile <- findInterval(t1$Prob,c(-Inf,decLocations, Inf))

t1_DA <-sqldf("select decile, count(decile) as count, min(Prob) as Min_prob
                     , max(Prob) as max_prob 
              , sum(cardio) as default_cnt
              from t1
              group by decile
              order by decile desc")

View(t1_DA) 
write.csv(t1_DA,"mydata1_DA.csv")


#Decile Analysis Reports - t2(testing)

# find the decile locations 
decLocations <- quantile(t2$Prob, probs = seq(0.1,0.9,by=0.1))

# use findInterval with -Inf and Inf as upper and lower bounds
t2$decile <- findInterval(t2$Prob,c(-Inf,decLocations, Inf))


t2_DA <- sqldf("select decile, count(decile) as count, min(Prob) as Min_prob
                     , max(Prob) as max_prob 
                    , sum(cardio) as default_cnt
                    from t2
                    group by decile
                    order by decile desc")
View(t2_DA)
```




